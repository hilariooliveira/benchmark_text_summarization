{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes xmltodict"
      ],
      "metadata": {
        "id": "aMcQl1uIy1nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ELLHOEp_-lYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "edJnXyLA-kSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xmltodict\n",
        "import sys\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def replace_marks(text: str) -> str:\n",
        "    text = text.replace('&amp;', '&')\n",
        "    text = text.replace('&quot;', '\"')\n",
        "    text = text.replace('&apost;', \"'\")\n",
        "    return text\n",
        "\n",
        "def read_cnn_corpus(corpus_dir: str) -> dict:\n",
        "    assert os.path.exists(corpus_dir)\n",
        "    documents_names = os.listdir(corpus_dir)\n",
        "    corpus = []\n",
        "    with tqdm(total=len(documents_names), file=sys.stdout, colour='blue',\n",
        "              desc='Reading Corpus') as pbar:\n",
        "      for doc_name in documents_names:\n",
        "          document_file = os.path.join(corpus_dir, doc_name)\n",
        "          with open(document_file, encoding='utf-8') as file:\n",
        "              xml_doc = xmltodict.parse(file.read())\n",
        "          xml_doc = xml_doc['document']\n",
        "          title = xml_doc['title']\n",
        "          title = replace_marks(title)\n",
        "          highlights_element = xml_doc['summaries']['highlights']\n",
        "          highlights = ''\n",
        "          for sentence_highlight in highlights_element['sentence']:\n",
        "              highlights += sentence_highlight['#text'] + '\\n'\n",
        "          highlights = highlights.strip()\n",
        "          highlights = replace_marks(highlights)\n",
        "          gold_standard_element = xml_doc['summaries']['gold_standard']\n",
        "          gold_standard = ''\n",
        "          if isinstance(gold_standard_element['sentence'], dict):\n",
        "              gold_standard += gold_standard_element['sentence']['#text']\n",
        "          else:\n",
        "              for sentence_gold_standard in gold_standard_element['sentence']:\n",
        "                  if isinstance(sentence_gold_standard, dict):\n",
        "                      gold_standard += sentence_gold_standard['#text'] + '\\n'\n",
        "          gold_standard = gold_standard.strip()\n",
        "          gold_standard = replace_marks(gold_standard)\n",
        "          article_element = xml_doc['article']\n",
        "          text = ''\n",
        "          for paragraph_element in article_element['paragraph']:\n",
        "              sentences_element = paragraph_element['sentences']['sentence']\n",
        "              if isinstance(sentences_element, list):\n",
        "                  for sentence_element in sentences_element:\n",
        "                      text += sentence_element['content'] + ' '\n",
        "              else:\n",
        "                  text += sentences_element['content'] + ' '\n",
        "          text = text.strip()\n",
        "          text = replace_marks(text)\n",
        "          doc_name = doc_name.replace('.xml', '').replace(\"'\", '').lower()\n",
        "          name = doc_name.replace(';', '').replace('&', '').replace('%', '').strip()\n",
        "          document = {\n",
        "              'name': name,\n",
        "              'title': title,\n",
        "              'highlights': highlights,\n",
        "              'gold_standard': gold_standard,\n",
        "              'text': text\n",
        "          }\n",
        "          corpus.append(document)\n",
        "          pbar.update(1)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def save_summary(document_name: str, summary: str, summaries_dir: str,\n",
        "                 summary_name: str):\n",
        "    document_dir = os.path.join(summaries_dir, document_name.lower())\n",
        "    os.makedirs(document_dir, exist_ok=True)\n",
        "    summary_path = os.path.join(document_dir, f'{summary_name}.txt')\n",
        "    with open(summary_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(summary)"
      ],
      "metadata": {
        "id": "9fl7PnTf9jF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_dir = f'/content/drive/My Drive/Experimentos/abs_summ_benchmark/corpora/cnn'\n",
        "\n",
        "summaries_dir = f'/content/drive/My Drive/Experimentos/abs_summ_benchmark/summaries/cnn'\n",
        "\n",
        "os.makedirs(summaries_dir, exist_ok=True)\n",
        "\n",
        "corpus_cnn = read_cnn_corpus(corpus_dir)"
      ],
      "metadata": {
        "id": "Uj7c9U6Z-ryS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total of Documents: {len(corpus_cnn)}')\n",
        "\n",
        "print(corpus_cnn[0]['text'])"
      ],
      "metadata": {
        "id": "UuYF2qw7AWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "LPGfZBfBy2T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'gemma_7b'\n",
        "\n",
        "model_path = 'google/gemma-7b-it'\n",
        "\n",
        "print(f'Model Name: {model_name} -- {model_path}')"
      ],
      "metadata": {
        "id": "bz5SitIyy-DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    'text-generation',\n",
        "    model=model_path,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map='auto',\n",
        "    max_length=2048,\n",
        "    eos_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "ekajaxm_DLq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature': 0.3})"
      ],
      "metadata": {
        "id": "9_0sW8G70XRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "ARTICLE: ```{text}```. Summarize the article in ```{num_sentences}``` SENTENCES.\n",
        "SUMMARY:\n",
        "\"\"\"\n",
        "\n",
        "print(f'Template: {template}')"
      ],
      "metadata": {
        "id": "Ofo2UVY2zHG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['text', 'num_sentences'])\n",
        "\n",
        "print(f'Prompt Template: {prompt}')\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "rCt7W5TjzzAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "with tqdm(total=len(corpus_cnn), file=sys.stdout,\n",
        "          colour='green', desc='Summarizing documents') as pbar:\n",
        "\n",
        "  for document in corpus_cnn:\n",
        "\n",
        "    num_sentences = len(document['highlights'].split('\\n'))\n",
        "\n",
        "    input_text = document['text']\n",
        "\n",
        "    document_dir = os.path.join(summaries_dir, document['name'].lower())\n",
        "    summary_path = os.path.join(document_dir, f'{model_name}.txt')\n",
        "\n",
        "    if os.path.exists(summary_path):\n",
        "      pbar.update(1)\n",
        "      continue\n",
        "\n",
        "    summary = llm_chain.run(\n",
        "        {\n",
        "            'num_sentences': num_sentences,\n",
        "            'text': input_text\n",
        "            }\n",
        "        )\n",
        "\n",
        "    summary = summary.split('SUMMARY:')[1].strip()\n",
        "\n",
        "    save_summary(document['name'], summary, summaries_dir, model_name)\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "N5kVnkLl0GB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cuT1kg9OIrxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}